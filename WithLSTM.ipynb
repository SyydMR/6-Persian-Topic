{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33e466ca-b84b-45b6-ba73-c550957b7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5620f939-32ee-4339-80f2-66b1471dac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.losses import CategoricalCrossentropy, CategoricalFocalCrossentropy\n",
    "from keras.metrics import Precision, Recall, CategoricalAccuracy, F1Score, TruePositives, TrueNegatives, FalseNegatives, FalsePositives\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abd1ead1-b50c-48e1-81e5-7e9fb4bd7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/Alireza-Akhavan/text-classification/raw/main/6-persian-topics.zip\"\n",
    "dataset = tf.keras.utils.get_file(\"6-persian-topics.zip\", url,\n",
    "                                    extract=True, cache_dir='.',\n",
    "                                    cache_subdir='')\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), '6-persian-topics')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "242a4b13-b3af-403d-846a-74c20681b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ry = []\n",
    "ryazi = os.path.join(dataset_dir, 'ریاضیات/')\n",
    "for r in os.listdir(ryazi):\n",
    "    ry.append(os.path.join(dataset_dir, 'ریاضیات/', r))\n",
    "beh = []\n",
    "behdasht = os.path.join(dataset_dir, 'بهداشت و سلامت/')\n",
    "for r in os.listdir(behdasht):\n",
    "    beh.append(os.path.join(dataset_dir, 'بهداشت و سلامت/', r))\n",
    "jog = []\n",
    "joghrafia = os.path.join(dataset_dir, 'جغرافیا و مکانها/')\n",
    "for r in os.listdir(joghrafia):\n",
    "    jog.append(os.path.join(dataset_dir, 'جغرافیا و مکانها/', r))\n",
    "fan = []\n",
    "fanavari = os.path.join(dataset_dir, 'فناوری و علوم کاربردی و تکنولوژی/')\n",
    "for r in os.listdir(fanavari):\n",
    "    fan.append(os.path.join(dataset_dir, 'فناوری و علوم کاربردی و تکنولوژی/', r))\n",
    "var = []\n",
    "varzesh = os.path.join(dataset_dir, 'ورزش/')\n",
    "for r in os.listdir(varzesh):\n",
    "    var.append(os.path.join(dataset_dir, 'ورزش/', r))\n",
    "di = []\n",
    "din = os.path.join(dataset_dir, 'دین و اعتقاد/')\n",
    "for r in os.listdir(din):\n",
    "    di.append(os.path.join(dataset_dir, 'دین و اعتقاد/', r))\n",
    "ry.extend(beh)\n",
    "ry.extend(jog)\n",
    "ry.extend(fan)\n",
    "ry.extend(var)\n",
    "ry.extend(di)\n",
    "\n",
    "for i in ry:\n",
    "    file = open(i, 'r')\n",
    "    content = file.read()\n",
    "    if len(content.split(' ')) <= 5:\n",
    "        os.remove(i)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19137536-7520-4efc-9760-87024985a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(dataset_dir)\n",
    "dataset = []\n",
    "labels = []\n",
    "for c in classes:\n",
    "    files = os.listdir(os.path.join(dataset_dir, c))\n",
    "    for f in files:\n",
    "        file_path = os.path.join(os.path.join(dataset_dir, c), f)\n",
    "        dataset.append(file_path)\n",
    "        labels.append(classes.index(c))\n",
    "\n",
    "\n",
    "train_data, valid_data, train_target, valid_target = train_test_split(dataset, labels, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8cf22a68-d804-4275-b93c-282b2fbcfca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    stripped_ye = tf.strings.regex_replace(stripped_html, 'ي', 'ی')\n",
    "    stripped_camma = tf.strings.regex_replace(stripped_ye, '،', ' ')\n",
    "    stripped_colon = tf.strings.regex_replace(stripped_camma, ':', ' ')\n",
    "    stripped_he = tf.strings.regex_replace(stripped_colon, 'هٔ', 'ه')\n",
    "    stripped_ke = tf.strings.regex_replace(stripped_he, 'ك', 'ک')\n",
    "    stripped_alef = tf.strings.regex_replace(stripped_ke, 'آ', 'ا')\n",
    "    stripped_english = tf.strings.regex_replace(stripped_alef, '[a-zA-Z]', ' ')\n",
    "    return tf.strings.regex_replace(stripped_english, '[%s]' % re.escape(string.punctuation), '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "51233c01-3e0b-42d6-bc8f-b4521178ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature = 20000\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=max_feature,\n",
    "    standardize=custom_standardization,\n",
    "    output_sequence_length=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2545707a-6208-437c-8c99-c9247f830f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def get_text_with_target(path, target):\n",
    "    target = tf.one_hot(target, depth=6, dtype='float32')\n",
    "    content = tf.io.read_file(path)\n",
    "    return content, target\n",
    "\n",
    "def vectorize_text(text, target):\n",
    "    text = vectorize_layer(text)\n",
    "    return text, target\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_target))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((valid_data, valid_target))\n",
    "\n",
    "train_dataset = train_dataset.map(get_text_with_target, num_parallel_calls=AUTOTUNE)\n",
    "vectorize_layer.adapt(train_dataset.map(lambda t, l: t, num_parallel_calls=AUTOTUNE))\n",
    "train_dataset = train_dataset.map(vectorize_text, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, num_parallel_calls=AUTOTUNE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "validation_dataset = validation_dataset.map(get_text_with_target, num_parallel_calls=AUTOTUNE).map(vectorize_text, num_parallel_calls=AUTOTUNE).cache().batch(BATCH_SIZE, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "52136cfd-18c0-40d6-bc7f-d0cdd935d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(input_dim=max_feature,\n",
    "                     output_dim=250),\n",
    "    layers.Bidirectional(layers.LSTM(128)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(classes), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4d436157-1a13-4d25-be79-6f11ad3df63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, None, 250)         5000000   \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 256)               388096    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5421766 (20.68 MB)\n",
      "Trainable params: 5421766 (20.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d04db3d8-3dfe-4713-8c21-c1a01595b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "              metrics=[\n",
    "                  tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "                  tf.keras.metrics.Precision(name='precision'),\n",
    "                  tf.keras.metrics.Recall(name='recall'),\n",
    "                  tf.keras.metrics.F1Score(name='f1-score')\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c2bf44e6-204b-4fe5-ac11-c2ca540c3fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class (0.0, 0.0, 0.0, 1.0, 0.0, 0.0): 4993 samples\n",
      "Class (0.0, 1.0, 0.0, 0.0, 0.0, 0.0): 17337 samples\n",
      "Class (0.0, 0.0, 0.0, 0.0, 1.0, 0.0): 1527 samples\n",
      "Class (0.0, 0.0, 1.0, 0.0, 0.0, 0.0): 5189 samples\n",
      "Class (1.0, 0.0, 0.0, 0.0, 0.0, 0.0): 304 samples\n",
      "Class (0.0, 0.0, 0.0, 0.0, 0.0, 1.0): 410 samples\n",
      "Class 0 weight: 5.960344482275185\n",
      "Class 1 weight: 1.7165599584703235\n",
      "Class 2 weight: 19.48919449901768\n",
      "Class 3 weight: 5.735209096164964\n",
      "Class 4 weight: 97.89473684210526\n",
      "Class 5 weight: 72.58536585365853\n"
     ]
    }
   ],
   "source": [
    "train_iterator = train_dataset.as_numpy_iterator()\n",
    "\n",
    "class_counts = {}\n",
    "class_indices = {}\n",
    "\n",
    "class_index = 0\n",
    "for data, labels in train_iterator:\n",
    "    for label in labels:\n",
    "        label_tuple = tuple(label)\n",
    "        if label_tuple not in class_counts:\n",
    "            class_counts[label_tuple] = 1\n",
    "            class_indices[label_tuple] = class_index\n",
    "            class_index += 1\n",
    "        else:\n",
    "            class_counts[label_tuple] += 1\n",
    "\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "class_weights = {\n",
    "    class_indices[label]: total_samples / count\n",
    "    for label, count in class_counts.items()\n",
    "}\n",
    "\n",
    "for label, count in class_counts.items():\n",
    "    print(f\"Class {label}: {count} samples\")\n",
    "\n",
    "for class_index, weight in class_weights.items():\n",
    "    print(f\"Class {class_index} weight: {weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c90f6d94-a7ef-4094-89b0-bb9d299e8397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "465/465 [==============================] - 54s 106ms/step - loss: 4.0110 - accuracy: 0.8904 - precision: 0.9286 - recall: 0.8734 - f1-score: 0.6540 - val_loss: 0.1260 - val_accuracy: 0.9665 - val_precision: 0.9753 - val_recall: 0.9622 - val_f1-score: 0.8375\n",
      "Epoch 2/20\n",
      "465/465 [==============================] - 30s 65ms/step - loss: 1.0652 - accuracy: 0.9748 - precision: 0.9794 - recall: 0.9722 - f1-score: 0.8710 - val_loss: 0.1767 - val_accuracy: 0.9561 - val_precision: 0.9635 - val_recall: 0.9509 - val_f1-score: 0.8250\n",
      "Epoch 3/20\n",
      "465/465 [==============================] - 27s 57ms/step - loss: 0.4554 - accuracy: 0.9857 - precision: 0.9886 - recall: 0.9840 - f1-score: 0.9272 - val_loss: 0.1614 - val_accuracy: 0.9637 - val_precision: 0.9680 - val_recall: 0.9606 - val_f1-score: 0.8739\n",
      "Epoch 4/20\n",
      "465/465 [==============================] - 26s 55ms/step - loss: 0.2784 - accuracy: 0.9889 - precision: 0.9908 - recall: 0.9880 - f1-score: 0.9415 - val_loss: 0.0964 - val_accuracy: 0.9766 - val_precision: 0.9803 - val_recall: 0.9745 - val_f1-score: 0.8884\n",
      "Epoch 5/20\n",
      "465/465 [==============================] - 25s 53ms/step - loss: 0.4492 - accuracy: 0.9873 - precision: 0.9887 - recall: 0.9863 - f1-score: 0.9400 - val_loss: 0.1631 - val_accuracy: 0.9711 - val_precision: 0.9734 - val_recall: 0.9698 - val_f1-score: 0.8842\n",
      "Epoch 6/20\n",
      "465/465 [==============================] - 25s 52ms/step - loss: 0.5958 - accuracy: 0.9825 - precision: 0.9846 - recall: 0.9814 - f1-score: 0.9283 - val_loss: 0.1989 - val_accuracy: 0.9686 - val_precision: 0.9712 - val_recall: 0.9667 - val_f1-score: 0.8860\n",
      "Epoch 7/20\n",
      "465/465 [==============================] - 24s 50ms/step - loss: 0.7935 - accuracy: 0.9770 - precision: 0.9796 - recall: 0.9754 - f1-score: 0.9150 - val_loss: 0.1787 - val_accuracy: 0.9624 - val_precision: 0.9692 - val_recall: 0.9587 - val_f1-score: 0.8604\n",
      "Epoch 8/20\n",
      "465/465 [==============================] - 24s 51ms/step - loss: 0.9574 - accuracy: 0.9754 - precision: 0.9798 - recall: 0.9725 - f1-score: 0.9140 - val_loss: 0.1863 - val_accuracy: 0.9663 - val_precision: 0.9710 - val_recall: 0.9643 - val_f1-score: 0.8688\n",
      "Epoch 9/20\n",
      "465/465 [==============================] - 24s 52ms/step - loss: 0.5449 - accuracy: 0.9841 - precision: 0.9875 - recall: 0.9824 - f1-score: 0.9301 - val_loss: 0.1644 - val_accuracy: 0.9702 - val_precision: 0.9764 - val_recall: 0.9669 - val_f1-score: 0.8799\n",
      "Epoch 10/20\n",
      "465/465 [==============================] - 24s 50ms/step - loss: 0.2740 - accuracy: 0.9877 - precision: 0.9908 - recall: 0.9863 - f1-score: 0.9503 - val_loss: 0.2084 - val_accuracy: 0.9630 - val_precision: 0.9671 - val_recall: 0.9609 - val_f1-score: 0.8593\n",
      "Epoch 11/20\n",
      "465/465 [==============================] - 24s 51ms/step - loss: 0.2689 - accuracy: 0.9888 - precision: 0.9904 - recall: 0.9878 - f1-score: 0.9531 - val_loss: 0.2583 - val_accuracy: 0.9497 - val_precision: 0.9555 - val_recall: 0.9484 - val_f1-score: 0.8085\n",
      "Epoch 12/20\n",
      "465/465 [==============================] - 24s 50ms/step - loss: 0.5148 - accuracy: 0.9840 - precision: 0.9863 - recall: 0.9828 - f1-score: 0.9403 - val_loss: 0.2218 - val_accuracy: 0.9620 - val_precision: 0.9666 - val_recall: 0.9597 - val_f1-score: 0.8626\n",
      "Epoch 13/20\n",
      "465/465 [==============================] - 24s 51ms/step - loss: 0.4348 - accuracy: 0.9867 - precision: 0.9882 - recall: 0.9856 - f1-score: 0.9444 - val_loss: 0.2566 - val_accuracy: 0.9597 - val_precision: 0.9646 - val_recall: 0.9571 - val_f1-score: 0.8694\n",
      "Epoch 14/20\n",
      "465/465 [==============================] - 24s 50ms/step - loss: 0.3476 - accuracy: 0.9867 - precision: 0.9886 - recall: 0.9853 - f1-score: 0.9439 - val_loss: 0.2614 - val_accuracy: 0.9652 - val_precision: 0.9684 - val_recall: 0.9645 - val_f1-score: 0.8767\n",
      "Epoch 15/20\n",
      "465/465 [==============================] - 24s 51ms/step - loss: 0.3466 - accuracy: 0.9894 - precision: 0.9912 - recall: 0.9884 - f1-score: 0.9605 - val_loss: 0.3244 - val_accuracy: 0.9577 - val_precision: 0.9607 - val_recall: 0.9550 - val_f1-score: 0.8613\n",
      "Epoch 16/20\n",
      "465/465 [==============================] - 23s 49ms/step - loss: 0.4436 - accuracy: 0.9865 - precision: 0.9878 - recall: 0.9854 - f1-score: 0.9491 - val_loss: 0.2394 - val_accuracy: 0.9629 - val_precision: 0.9669 - val_recall: 0.9616 - val_f1-score: 0.8536\n",
      "Epoch 17/20\n",
      "465/465 [==============================] - 23s 49ms/step - loss: 0.5936 - accuracy: 0.9877 - precision: 0.9892 - recall: 0.9870 - f1-score: 0.9523 - val_loss: 0.3813 - val_accuracy: 0.9540 - val_precision: 0.9556 - val_recall: 0.9538 - val_f1-score: 0.8058\n",
      "Epoch 18/20\n",
      "465/465 [==============================] - 23s 49ms/step - loss: 0.4996 - accuracy: 0.9852 - precision: 0.9868 - recall: 0.9844 - f1-score: 0.9442 - val_loss: 0.2110 - val_accuracy: 0.9597 - val_precision: 0.9647 - val_recall: 0.9573 - val_f1-score: 0.8529\n",
      "Epoch 19/20\n",
      "465/465 [==============================] - 23s 49ms/step - loss: 0.4533 - accuracy: 0.9870 - precision: 0.9892 - recall: 0.9859 - f1-score: 0.9469 - val_loss: 0.2625 - val_accuracy: 0.9495 - val_precision: 0.9583 - val_recall: 0.9449 - val_f1-score: 0.8418\n",
      "Epoch 20/20\n",
      "465/465 [==============================] - 23s 49ms/step - loss: 0.5694 - accuracy: 0.9844 - precision: 0.9857 - recall: 0.9829 - f1-score: 0.9423 - val_loss: 0.2550 - val_accuracy: 0.9570 - val_precision: 0.9593 - val_recall: 0.9546 - val_f1-score: 0.8518\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=20, validation_data=validation_dataset, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e1b4329d-09a3-46b5-b0e5-ecd3aff6cd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 3s 17ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.56      0.70        72\n",
      "           1       0.99      0.96      0.98      4382\n",
      "           2       0.91      0.95      0.93      1243\n",
      "           3       0.97      0.97      0.97      1269\n",
      "           4       0.90      0.94      0.92       367\n",
      "           5       0.48      0.85      0.61       108\n",
      "\n",
      "    accuracy                           0.96      7441\n",
      "   macro avg       0.86      0.87      0.85      7441\n",
      "weighted avg       0.96      0.96      0.96      7441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(validation_dataset) \n",
    "predicted_classes = tf.math.argmax(predictions, axis=-1)\n",
    "from sklearn.metrics import classification_report \n",
    "report = classification_report(valid_target, predicted_classes) \n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4ccae-b68b-4f32-8a57-d4d4ccbee7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
